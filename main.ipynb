{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    '''this method is used to convert a given text to tokens by changing all letters to lowercase letters\n",
    "        and remove numbers. It also ignores words containing numbers'''\n",
    "    with open('stopwordlist.txt', 'r') as swFile:#loading all the stopwrds from stopwords.txt file\n",
    "        stopwords = re.split(r'\\s+', swFile.read())\n",
    "\n",
    "    tokens = []\n",
    "    for line in text:\n",
    "        tokens += [token for token in re.split('[^a-zA-Z0-9]', line.lower()) if\n",
    "                    not re.search('[0-9]', token) and token not in stopwords] # to perform tokenization on non-alphanumeric words, to remove empty words from the list, to ignore words containing numbers \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "all_words = []\n",
    "\n",
    "class WordDictionary:\n",
    "    #first we define a constructor to initialize our variables\n",
    "    def __init__(self):\n",
    "        self.wordIDs = {} #dict for all word ids\n",
    "        self.currWordId = 1 #start with id 1 for words\n",
    "        self.stemmer = PorterStemmer() #initalize stemmer obj\n",
    "\n",
    "    def appendWord(self, word):\n",
    "        stemWord = self.stemmer.stem(word) #setup the stem word\n",
    "        if stemWord == \"\":\n",
    "            pass\n",
    "        elif stemWord not in self.wordIDs: #check if the dict contains the word\n",
    "            self.wordIDs[stemWord] = self.currWordId #if the word is not present we add the word to the dict along with the id\n",
    "            self.currWordId += 1 #increment the id for the next word\n",
    "\n",
    "    def getWordId(self, word):\n",
    "        stemWord = self.stemmer.stem(word)#getting the stemmed form of the given word\n",
    "        return self.wordIDs.get(stemWord, None)#returning the ID of the stemmef word\n",
    "    \n",
    "    def fetch_d(self):#this method will be used in main.py to fetch all ids that will be written to the output file\n",
    "        sorted_order = dict(sorted(self.wordIDs.items(), key=lambda item:item[1]))#sorting the word dictionary based on WordIDs\n",
    "        return sorted_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './ft911/'\n",
    "all_file_names = []\n",
    "\n",
    "class FileDictionary:\n",
    "    #similar to WordDictionary we do the initialize step\n",
    "    def __init__(self, path):\n",
    "        self.fileIDs = {} #dict for all file ids\n",
    "        self.currFileId = 1#start with id 1 for files\n",
    "        self.folder = path#set a foldder ref with the given path\n",
    "\n",
    "    def getFileId(self, file):\n",
    "        return self.fileIDs.get(file, None)#return the ID of the file(DOCNO) for a given file\n",
    "    \n",
    "    def getAllFiles(self):\n",
    "        return self.fileIDs#returns all the files(DOCNOS)\n",
    "    \n",
    "    \n",
    "    def appendFiles(self, file):\n",
    "         self.fileIDs[file] = self.currFileId#appending the file(docno) along with a new id\n",
    "         self.currFileId += 1#incrementing the id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.dom.minidom as xdm\n",
    "\n",
    "date = 'DATE'\n",
    "pro = 'PROFILE'\n",
    "DOC = 'DOCNO'\n",
    "TEXT = 'TEXT'\n",
    "path = './ft911/'\n",
    "doc_tag = ['<DOC>', '</DOC>']\n",
    "read_mode = 'r'\n",
    "\n",
    "class TextParser(object):\n",
    "    def convert(val):#method will be used by the fetchdocs() method below to fetch all docnos and complete data\n",
    "        doc = xdm.parseString(val)#using xdm library to manipulate/parse the input files\n",
    "        root = doc.documentElement#gets the root element\n",
    "        docs = {}#declaring a dict to store all data\n",
    "\n",
    "        for doc in root.childNodes:#iterating through all docs\n",
    "            for ele in doc.childNodes:#iiterating through all elements in docs\n",
    "                if(ele.nodeType == ele.ELEMENT_NODE):#check if the nodetype of the element is node element itself\n",
    "                    if ele.tagName == date or ele.tagName == pro:#ignoring all elements that are not docno\n",
    "                        continue\n",
    "                    elif ele.tagName == DOC:#if the element is docno then we add it to out dict\n",
    "                        DOCNO = ele.firstChild.data.strip()\n",
    "                        docs[DOCNO] = []\n",
    "                    elif ele.tagName == TEXT:#else we append all the data to that specific docno\n",
    "                        docs[DOCNO].append(ele.firstChild.data.strip())\n",
    "        return docs\n",
    "    \n",
    "    \n",
    "    def fetchDocs(file):#this method will be used in main.py to get the parsed docnos and doc content\n",
    "        with open(os.path.join(path, file), read_mode) as File:\n",
    "            doc_data = File.read()\n",
    "        full_doc_data = doc_tag[0] + doc_data + doc_tag[1]\n",
    "        docs = TextParser.convert(full_doc_data)\n",
    "        return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "\n",
    "    def create_forward_index(index):\n",
    "        new_forwardIndex = {}\n",
    "        for docno, text in index.items():\n",
    "            word_counts = Counter(text)\n",
    "            new_forwardIndex[docno] = word_counts\n",
    "        return new_forwardIndex\n",
    "        \n",
    "\n",
    "    def write_forward_index(new_forwardIndex, fileName):    \n",
    "        try:\n",
    "            filepath = os.path.join(Path(__file__).parent.resolve(), fileName)\n",
    "            with open(filepath, \"w\") as fIndex:\n",
    "                for docno, counts in new_forwardIndex.items():\n",
    "                    fIndex.write(f\"{docno}:\\t\" +\"; \".join([f\"{word} {count}\" for word, count in counts.items()]))\n",
    "                    fIndex.write(\"\\n\\n\")\n",
    "            print(\"index success in \"+fileName)\n",
    "        except:\n",
    "            print(f\"index {fileName} failure \")\n",
    "\n",
    "\n",
    "        \n",
    "    def create_inverted_index(docs):\n",
    "        inverted_index = {}\n",
    "        for docno, wordCounts in docs.items():\n",
    "            for word, count in wordCounts.items():\n",
    "                if word not in inverted_index:\n",
    "                    inverted_index[word] = []\n",
    "                inverted_index[word].append((docno,count))\n",
    "        # print(inverted_index)\n",
    "        return inverted_index\n",
    "    \n",
    "    def write_inverted_index(inverted_index, filename):\n",
    "        try:\n",
    "            filepath = os.path.join(Path(__file__).parent.resolve(), filename)\n",
    "            with open(filepath, \"w\") as iIndex:\n",
    "                for word, postings in inverted_index.items():\n",
    "                    iIndex.write(f\"{word}:\\t\"+\"; \".join(f\"{docno} {count}\" for docno, count in postings))\n",
    "                    iIndex.write(\"\\n\\n\")\n",
    "            print(\"index success in \"+filename)\n",
    "        except:\n",
    "            print(f\"index {filename} failure \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code at: 2024-03-28 13:11:50.422704\n",
      "Parser output- Success: Writing data COmpleted!\n",
      "Parser output- Success: Writing data COmpleted!\n",
      "Time taken for Parsing the documents = 0.038260459899902344 seconds\n",
      "index success in forward_index.txt\n",
      "index success in inverted_index.txt\n",
      "Time taken to generate the indexes is 0.7784864902496338 seconds.\n",
      "Total number of words = 32606\n",
      "Total number of documents = 5368\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from WordDictionary import WordDictionary\n",
    "from FileDictionary import FileDictionary\n",
    "from tokenizer import tokenizer\n",
    "from DocParser import TextParser\n",
    "from datetime import datetime\n",
    "from indexer import Indexer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "print(\"Running code at:\", datetime.now())\n",
    "\n",
    "path = \"./ft911/\"\n",
    "folder = os.listdir(path=path)\n",
    "\n",
    "parser_file = \"parser_output.txt\"\n",
    "forward_index_file = \"forward_index.txt\"\n",
    "inverted_index_file = \"inverted_index.txt\"\n",
    "w = 'w'\n",
    "a = 'a'\n",
    "\n",
    "def parser_output(filename, data, mode):    \n",
    "    try:#exception handling\n",
    "        # filePath = os.path.join(Path(__file__).parent.resolve(), parser_file)#created a file if it doesn't already exist to write output\n",
    "        with open(filename, mode) as parser_output:\n",
    "            for key in data.keys():#iterates through all docnos and stemmed words\n",
    "                parser_output.write(f\"{key}\\t{data[key]}\\n\")#appeninf the DOCNO: DOCID and Word : WOrdID to the parser_output file\n",
    "        print(\"Parser output- Success: Writing data COmpleted!\")\n",
    "    except :#shows error message in case of failure\n",
    "        print(\"Parser output- Failure: error while uploading data\")\n",
    "\n",
    "\n",
    "#then we initialize our custom dictionaries\n",
    "WordDict = WordDictionary()\n",
    "FileDict = FileDictionary(path)\n",
    "\n",
    "def get_stem_words(forward_words):\n",
    "    stemmer = PorterStemmer() \n",
    "    stemmed_words = []\n",
    "    for word in forward_words:\n",
    "        stemword = stemmer.stem(word)\n",
    "        if stemword == \"\":\n",
    "            pass\n",
    "        elif stemword not in stemmed_words:\n",
    "            stemmed_words.append(stemword)\n",
    "    return stemmed_words\n",
    "\n",
    "forwardIndex = {}\n",
    "forward_tokens = []\n",
    "\n",
    "#looping over all the files in the folder mentioned above and and adding them to complete data\n",
    "for file in folder:\n",
    "    docs = TextParser.fetchDocs(file)\n",
    "    for docno, data in docs.items():\n",
    "        FileDict.appendFiles(docno)\n",
    "        tokens = tokenizer(data)\n",
    "        for token in tokens:\n",
    "            WordDict.appendWord(token)\n",
    "        stemmed_words = get_stem_words(tokens)\n",
    "        forwardIndex[docno] = stemmed_words\n",
    "\n",
    "\n",
    "start_parser = time.time()\n",
    "parser_output(parser_file, WordDict.fetch_d(), w)#once all the word data is feteched, we write the data to the output file using the 'w' mode\n",
    "parser_output(parser_file, FileDict.getAllFiles(), a)#once all the filenames are feteched, we append the data to the output file using the 'a' mode\n",
    "end_parser = time.time()\n",
    "\n",
    "total_parser_time = end_parser - start_parser\n",
    "print(f\"Time taken for Parsing the documents = {total_parser_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "new_forwardIndex = Indexer.create_forward_index(forwardIndex)\n",
    "Indexer.write_forward_index(new_forwardIndex, forward_index_file)\n",
    "\n",
    "inverted_index = Indexer.create_inverted_index(new_forwardIndex)\n",
    "Indexer.write_inverted_index(inverted_index, inverted_index_file)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken to generate the indexes is {execution_time} seconds.\")\n",
    "print(f\"Total number of words = {len(inverted_index)}\")\n",
    "print(f\"Total number of documents = {len(new_forwardIndex)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
